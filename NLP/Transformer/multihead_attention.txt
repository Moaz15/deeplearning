===========================================
MULTI-HEAD ATTENTION 
===========================================

1. Why Multi-Head Attention (MHA) Is Needed
-------------------------------------------

Self-Attention gives each word a contextual meaning by looking at **all** other words.
But using ONLY ONE attention head has a major limitation:

    → A single head can focus on only **one type of relationship** at a time.

Example:
Sentence: "the man saw a kid with a telescope"

Possible interpretations:
1) The man used the telescope.  
2) The kid had the telescope.

A single attention head cannot capture BOTH interpretations simultaneously.

In natural language:
- Words relate in many ways (syntax, semantics, coreference, objects, modifiers)
- A single attention pattern is too restrictive

Multi-Head Attention solves this by allowing the model to look at the sentence from multiple perspectives.

-------------------------------------------------------------------------------------
2. What Multi-Head Attention Does


Instead of computing ONE Self-Attention output (one perspective),
the Transformer computes **multiple attention operations in parallel**.

Each one is called a HEAD.

Example: 8 heads in the original Transformer  
(each head learns a different “view” of the sentence)

For each head h:
- It has its OWN learned projection matrices:
      W_Qʰ, W_Kʰ, W_Vʰ
- It computes its own:
      Qʰ, Kʰ, Vʰ → Attention → Zʰ (head output)

So each head focuses on a different aspect:
- Head 1: subject ↔ verb relation
- Head 2: noun ↔ adjective
- Head 3: long-range dependency
- Head 4: coreference (“he”, “the man”)
- …

Different heads discover different linguistic patterns automatically.


3. How Multi-Head Attention Works (High-Level)
-------------------------------------------

Step 1 — Split embedding dimension  
-----------------------------------
Given embedding size = 512  
and number of heads = 8:

Each head gets:
    d_head = 512 / 8 = 64 dimensions

This allows heads to be cheaper and specialized.

Step 2 — Linear projections per head  
------------------------------------
For each head h:

    Qʰ = X × W_Qʰ
    Kʰ = X × W_Kʰ
    Vʰ = X × W_Vʰ

Each W matrix is 512×64  
Each head learns DIFFERENT W_Q, W_K, W_V  
→ Different views of the same input.

Step 3 — Self-Attention inside each head  
----------------------------------------
Each head computes:

    Zʰ = Attention(Qʰ, Kʰ, Vʰ)

This produces a 2×64 vector per head (for 2 tokens).

Step 4 — Concatenate all head outputs  
-------------------------------------
[Z¹, Z², Z³, ... Z⁸]  
→ Combined to form a 2×512 matrix.

Step 5 — Final linear projection  
---------------------------------
The concatenated output passes through:
    Wᴼ   (a 512×512 matrix)

Giving the final output:
    Z_final = Concat(Z¹…Z⁸) × Wᴼ   → shape (2×512)

This puts information from all heads back into the model’s dimension.

-------------------------------------------
4. Why Do We Need Multiple Heads?
-------------------------------------------

Reason 1 — Different Perspectives  
    Each head focuses on a different relationship.
    Language has MANY relationships; one head cannot capture all.

Reason 2 — Ambiguity Resolution  
    “the man saw a kid with a telescope”
    - One head interprets “man used telescope”
    - Another interprets “kid had telescope”
    Both interpretations are preserved.

Reason 3 — Subspace Specialization  
    Splitting 512 → 8×64 forces each head to learn a different subspace:
        syntax
        semantics
        grammar
        objects
        pronouns
        long-range links

Reason 4 — Improves Model Expressiveness  
    Multiple heads create richer, more diverse attention patterns.

Reason 5 — Stability & Learning Efficiency  
    Smaller heads are easier to train.
    Many small heads > one huge head.

-------------------------------------------
5. Summary (for interviews & revision)
-------------------------------------------

Multi-Head Attention = multiple Self-Attention units running in parallel.

Each head:
    - has its own W_Q, W_K, W_V
    - learns a different linguistic pattern
    - outputs a 64-dimensional vector

All heads together:
    - capture different relationships
    - resolve ambiguities
    - create a richer representation

Final step:
    - concatenate heads
    - project back to 512-dim using Wᴼ

This makes the Transformer flexible, powerful, and good at understanding complex language structures.

===========================================
2. Multi-Head Attention — Dimension Flow (With Shapes)
===========================================

We start with:
    Sequence length = N
    Embedding size = d_model
    Number of heads = h
    Per-head dimension = d_head = d_model / h

Example (Transformer):
    d_model = 512
    h = 8
    d_head = 512 / 8 = 64


-------------------------------------------
Step 1 — Input Embeddings (X)
-------------------------------------------

X has shape:
    (N × d_model)

Example:
    2 tokens → shape = (2 × 512)


-------------------------------------------
Step 2 — Linear Projections for Each Head
-------------------------------------------

For each head h, we learn:
    W_Qʰ : (d_model × d_head)
    W_Kʰ : (d_model × d_head)
    W_Vʰ : (d_model × d_head)

We compute:
    Qʰ = X × W_Qʰ     → shape (N × 64)
    Kʰ = X × W_Kʰ     → shape (N × 64)
    Vʰ = X × W_Vʰ     → shape (N × 64)

So each head produces:
    Qʰ, Kʰ, Vʰ → (N × 64)


-------------------------------------------
Step 3 — Scaled Dot-Product Attention per Head
-------------------------------------------

Attention computes:

    scores = Qʰ × (Kʰ)ᵀ            → (N × N)
    weights = softmax(scores)      → (N × N)
    Zʰ = weights × Vʰ              → (N × 64)

Zʰ is the output of head h.


-------------------------------------------
Step 4 — Concatenate All Heads
-------------------------------------------

We have h heads:
    Z¹, Z², ..., Zʰ

Each has shape:
    (N × 64)

Concatenation:
    Concat(Z¹…Zʰ)  → shape (N × (h × d_head))
                    → shape (N × 512)


-------------------------------------------
Step 5 — Final Linear Projection (Wᴼ)
-------------------------------------------

Wᴼ has shape:
    (d_model × d_model)
    (512 × 512)

Final output:
    Z_final = Concat(Z¹…Z⁸) × Wᴼ

Shape:
    (N × 512)


-------------------------------------------
Full Dimension Summary
-------------------------------------------

X               : (N × 512)
W_Q, W_K, W_V   : (512 × 64)
Q, K, V         : (N × 64)
scores          : (N × N)
weights         : (N × N)
Zʰ (head output): (N × 64)
Concat          : (N × 512)
Wᴼ              : (512 × 512)
Z_final         : (N × 512)

-------------------------------------------
Key Insight

Multi-Head Attention:
- splits the model dimension into smaller subspaces (64 dim per head)
- performs attention independently in each subspace
- recombines them back into the full 512-dimensional space

===========================================
END OF SECTION 2
=======================================


===========================================
3. Why Splitting Improves Learning (Intuition)
===========================================

A single attention head sees the entire embedding space (512 dimensions)
and produces one attention pattern.  
Multi-Head Attention SPLITS this space into smaller subspaces:

    512 → 8×64  (for 8 heads)

Each head gets its own “view” of the sentence and learns something different.
This dramatically improves the model’s ability to understand language.

Here are the core reasons:


-------------------------------------------
Reason 1 — Multiple Perspectives
-------------------------------------------

Language is multi-dimensional.  
A single attention head can focus on only ONE type of relationship at a time.

Examples of relationships:
    - subject ↔ verb
    - adjective ↔ noun
    - pronoun ↔ noun (coreference)
    - object ↔ action
    - long-range dependencies
    - negation (“not”)
    - entity boundaries
    - syntax vs semantics

Splitting allows each head to specialize:
    Head 1 → grammar
    Head 2 → meaning
    Head 3 → coreference
    Head 4 → long-range links
    ...

One head = one perspective  
Many heads = many perspectives simultaneously


-------------------------------------------
Reason 2 — Ambiguity Handling
-------------------------------------------

Sentence:
    “The man saw a kid with a telescope.”

Two possible meanings:
    1) The man had the telescope.
    2) The kid had the telescope.

One head cannot capture both interpretations.  
Multi-Head Attention allows:

    Head 1 → focuses on (man ↔ telescope)
    Head 2 → focuses on (kid ↔ telescope)

The model keeps BOTH meanings alive until later layers decide.


-------------------------------------------
Reason 3 — Subspace Specialization
-------------------------------------------

Splitting 512 dimensions into 64-dim chunks forces each head to learn
a different aspect of the embedding space.

Think of a huge problem broken into smaller, simpler ones:
    512-dim problem → 8 × 64-dim problems

Each 64-dim subspace is:
    - simpler
    - easier to optimize
    - faster to learn
    - better at capturing a specific linguistic pattern

This divide-and-conquer strategy is one of the biggest reasons
Transformers train stably and efficiently.


-------------------------------------------
Reason 4 — Reduced Interference
-------------------------------------------

In one large head, all patterns compete inside the same 512 dimensions.
Patterns can interfere with each other.

In multi-heads:
    Each head learns independently.
    No interference.
    Gradients don’t clash.

This isolation makes learning cleaner and more structured.


-------------------------------------------
Reason 5 — Increased Model Expressiveness
-------------------------------------------

A single attention head = linear projection + dot-product attention  
Multiple heads = mixture of many different linear subspaces

This dramatically increases:
    - expressive power
    - representational richness
    - pattern diversity

Simply put:
    Many small heads > One big head

The combined output captures FAR more information than
a single attention map ever could.


-------------------------------------------
Reason 6 — Empirical Result (the real reason)
-------------------------------------------

The Transformer paper tried:
    - 1 head  
    - 2 heads  
    - 4 heads  
    - 8 heads  
    - 16 heads  

And found:
    → performance increases with number of heads  
    → until ~8 heads (for 512-d model)

Too many heads = each head becomes too small  
Too few heads = not enough perspectives


-------------------------------------------
Summary (one-line)
-------------------------------------------

Splitting attention creates multiple specialized “views” of the sentence,
making the Transformer far better at capturing complex and ambiguous language patterns.


1. Why Positional Encoding is Needed
Self-Attention is extremely powerful, but it has one major limitation:

Self-Attention has no understanding of word order.
It treats the input sentence as a bag of words.

Example:

“Moaz killed Lion”
“Lion killed Moaz”

Self-Attention sees the same set of words:
{Moaz, killed, Lion}

but does not know their order, because attention compares all words with all other words simultaneously.
This leads to a huge problem:
❌ Without position information, both sentences look identical.
But their meanings are opposite.

Why does Self-Attention lose order?

Because in the attention mechanism:
Q, K, and V depend only on the word embeddings.
Attention compares every word with every other word.
Nothing tells the model “this is the first word”, “this is the next word”, etc.

This means Self-Attention cannot naturally understand:

subject vs object
who did what to whom
whether “not” modifies the verb
or any structure based on order

Attention is position-invariant.

This invariance is powerful for meaning, but disastrous for sequence structure.

Why Simple Numbering (1, 2, 3, …) Doesn’t Work

We might think:
“Let’s just give each word its index!”

Moaz = position 1  
killed = position 2  
Lion = position 3

But this solution has three fundamental limitations, each of which makes it unusable for a real Transformer.

Problem 1 — Unbounded Sequence Length
Absolute numbering grows without any limit:

position 1
position 2
position 100
position 10,000
position 100,000

A Transformer trained on sentences up to length 1,024 would suddenly encounter:

Position 2048
Position 5000
etc.

Absolute numbers do not generalize because the model never saw such large values during training.

You want a positional representation that:

works for unseen sequence lengths
behaves smoothly even when length increases
does not blow up numerically

Problem 2 — Too Discrete (No Smoothness)

Attention computations are continuous operations:

dot products
softmax
weighted sums

But absolute positions like 1, 2, 3, 4… are discrete jumps.
Self-Attention cannot infer any meaningful geometric or relative structure from raw integers.
There is:
no smooth transition
no pattern
no periodicity
no generalizable structure

They fail to interact nicely with Q, K, V projections.

Problem 3 — No Sense of Relative Positioning

This is the most important failure.
Absolute positions tell you:
“Moaz is at position 1
killed is at position 2
Lion is at position 3”

But they do NOT tell the model:
“Moaz comes before killed”
“Moaz is 1 step away from killed”
“Lion is after killed”
“killed is the center of the event”
“Moaz and Lion are separated by one position”

And relative distance is exactly what determines meaning in language.
Meaning depends on relative order, not absolute indices.
Absolute numbering cannot encode relative relationships.

The distance between words matters more than their fixed positions.
Sentences of different lengths shift absolute positions.
So we need something better than raw indexes.
We need a learnable or mathematical encoding that embeds position in a meaningful, comparable way.

We need something continuous, smooth, pattern-based, and generalizable.

2. Sinusoidal Positional Encoding (Original Transformer Method)

Since absolute numbering fails to represent order meaningfully, the Transformer introduces a continuous, smooth, generalizable mathematical representation of position using:

✔ Sine waves
✔ Cosine waves
✔ Across multiple frequencies

This approach solves all the problems of absolute indexing.

Why Use Trigonometric Functions?
Sine and cosine waves have several powerful properties:

A. Continuous, Smooth Signals
Unlike discrete jumps in 1, 2, 3, …
sine/cosine vary smoothly and continuously:

This interacts beautifully with:
dot products
softmax
linear projections

B. Unbounded Sequence Length
A sine wave continues forever.
sin(x) works for:
10
100
10,000
100,000

No retraining needed.
Absolute positions cannot do this.

C. Encodes Relative Positioning Naturally

This is the most important feature.
The difference between two positions can be recovered from their sine/cosine patterns.
The Transformer can compute:
distance between words
ordering
layering
adjacency
far vs near
all from the wave patterns alone.

Why One Sine Wave Is Not Enough
A single sine wave repeats every 2π:
sin(θ) = sin(θ + 2π)
So: different positions map to the same value ->causing collisions ->misleading the model

pos 1 → sin(1)  
pos 7 → sin(7)  (7 ≈ 1 + 2π)

Both produce the same value. This makes one sine wave unusable.

Why Use Multiple Frequencies (High → Low)

Transformers use many sine waves with different wavelengths:
short wavelength → captures fine-grained differences
long wavelength → captures long-range structure

This ensures:
uniqueness of each position
expressiveness
both local and global relational information

Think of it like:

one wave captures “tiny shifts”
another captures “medium shifts”
another captures “big shifts”
all combined → perfect positional fingerprint

Why Use Cosine Along With Sine?

Reason 1 — Phase Shift
Cosine is just sine shifted by 90°:
This gives a richer representation.
The model gets two independent but related signals.

Reason 2 — Completeness
Sine and cosine together form a full basis for representing periodic patterns.

Reason 3 — More Distinguishability
Two dimensions capture more pattern variation than one.


The Final Sinusoidal Encoding Formula
For embedding dimension dmodel :
Even indices:


Odd indices:






TRANSFORMER ENCODER 
===============================================================================================================

1. Encoder Overview

The Transformer encoder consists of N identical layers (N = 6 in the paper).
Each encoder layer has two main sublayers:
    1) Multi-Head Self-Attention
    2) Feed Forward Neural Network (FFN)

Each sublayer is wrapped with:
    - Skip Connection (Residual)
    - Layer Normalization (Add & Norm)

So one encoder layer looks like:
    Input → Multi-Head Attention → Add & Norm → Feed Forward → Add & Norm → Output


All encoder layers have the same structure but different learned parameters.

2. Encoder Layer Flow (Step-by-Step)
===========================================

Let X be the input embeddings + positional encodings.

Step 1 — Multi-Head Self-Attention
-------------------------------------------

X → MHA → Z

MHA allows every token to attend to ALL other tokens in the sentence.

Output shape:
    Z : (sequence_length × d_model)


Step 2 — Add & Norm (Residual + LayerNorm)
-------------------------------------------

We add the original input X to the MHA output Z:

    Z_res = X + Z

Then apply Layer Normalization:

    Z_norm = LayerNorm(Z_res)

This normalized output goes into the feed-forward network.

-------------------------------------------
Step 3 — Feed Forward Network (FFN)
-------------------------------------------

Each token is passed independently through a 2-layer MLP:

    FFN(t) = ReLU(t W₁ + b₁) W₂ + b₂

Typical dimensions:
    W₁: (d_model × 2048)
    W₂: (2048 × d_model)

This adds non-linearity and feature transformation.

-------------------------------------------
Step 4 — Add & Norm (Second Residual Block)
-------------------------------------------

Add skip connection again:

    Y_res = Z_norm + FFN_output

Apply LayerNorm:

    Y_norm = LayerNorm(Y_res)

This Y_norm is the output of the encoder layer.
It becomes input to the next encoder layer.

===========================================
3. Why Layer Normalization?
===========================================

LayerNorm stabilizes and accelerates training by normalizing
across the feature dimension of each token.

    For each token’s vector (size = d_model):
        subtract mean
        divide by standard deviation
        scale & shift (learnable γ, β)

Benefits:

✔ Prevents exploding/vanishing values  
✔ Makes learning stable across deep layers 
✔ Ensures each token stays in a smooth range
✔ Prevents attention outputs from dominating FFN outputs
✔ Helps gradient flow during backpropagation

LayerNorm is applied AFTER the residual addition:
    Add → Norm  (original Transformer)
because it normalizes the combined signal and stabilizes stacking.


4. Why Skip Connections (Residual Connections)?
===================================================================================

Residual connections add the original input back to the sublayer output:

    Output = Input + Sublayer(Input)

Why?

Reason 1 — Prevent Vanishing Gradients  
--------------------------------------
Gradients can flow directly through the skip path.
This allows very deep models (6, 12, 24, 48 layers) to train reliably.

Reason 2 — Information Preservation  
------------------------------------
If the sublayer fails to learn something, the input is still preserved.

Reason 3 — Helps in Identity Mapping  
-------------------------------------
Early in training, the network learns to copy inputs → easier optimization.

Reason 4 — Combine Old + New Features  
-------------------------------------
Residuals allow:
    - original representation (X)
    - refined representation (Attention or FFN output)
to mix together.

This fusion gives richer meaning.

Reason 5 — Stabilizes Deep Stacking  
------------------------------------
Stacking 6–48 encoder layers becomes possible ONLY due to skip connections.


FEED-FORWARD NETWORK (FFN) — DEEP DIVE
===========================================

1. What the FFN Does
-------------------------------------------

Inside each encoder (and decoder) layer,
AFTER Multi-Head Attention, we apply a position-wise feed-forward network.

This network processes **each token independently** but identically:

    FFN(x) = ReLU(x W₁ + b₁) W₂ + b₂

It is simply a 2-layer MLP applied to every token vector.

Purpose:
    - Add non-linearity
    - Expand & compress features
    - Allow deeper abstraction
    - Enable richer transformations than attention alone


2. Why Not Just Use More Attention Layers?
===========================================

Attention captures **relationships between words**.

But attention alone is linear:
    Q, K, V projections are linear ops.
    Attention weighted sums are linear.

Without FFN, the Transformer would be almost linear → too weak.

FFN introduces:
    ✔ non-linearity  
    ✔ multi-layer learning  
    ✔ complex feature transformations  


3. FFN Dimensions (Key Insight)
-------------------------------------------

For d_model = 512, the FFN uses:

    W₁ : (512 × 2048)
    W₂ : (2048 × 512)

This is called:
    → expansion (512 → 2048)
    → compression (2048 → 512)

Why expand to 2048?

Because expanding dimensions lets the network learn **powerful intermediate features** before compressing back.

Think of it like:
    512-dim tokens go into a large workspace (2048 dims)
    → network learns rich, abstract patterns
    → compresses them back into 512 dims


4. FFN Intuition (How It Really Helps)
-------------------------------------------

Attention mixes information **across tokens**.

FFN transforms information **within each token**.

Together they work like:
    Attention = How tokens relate to each other
    FFN = How each token transforms internally

Example:
Token “bank” after attention already knows:
    “money” is nearby
    “flows” is related
    context determines meaning

But FFN helps refine:
    - semantic meaning
    - syntactic structure
    - task-specific features

The FFN lets the model detect:
    - higher-level patterns
    - abstract meaning (financial bank vs river bank)
    - transformations attention can’t capture alone

5. Why ReLU?
-------------------------------------------

ReLU adds:
    ✔ non-linearity (required for expressive power)
    ✔ sparsity (helps specialization)
    ✔ stable gradients

Sometimes GELU (used in GPT, BERT) works better because:
    - smoother
    - captures subtle patterns

But the original Transformer uses ReLU.


6. FFN Runs Independently for Each Token
-------------------------------------------

Important point:
The FFN does NOT allow tokens to talk to each other.

It is applied identically to every token vector.

Parallelizable? YES.
Simple? YES.
Powerful? VERY.

7. FFN Computation (Step-by-Step With Shapes)
-------------------------------------------

Let x be a token embedding (1 × 512).

Step 1: Expand  
    h = x W₁ + b₁        → shape (1 × 2048)

Step 2: Non-linearity  
    h_relu = ReLU(h)     → shape (1 × 2048)

Step 3: Compress  
    y = h_relu W₂ + b₂   → shape (1 × 512)

Each token independently produces a new 512-d vector.


8. Why 2048? Why Expansion?
-------------------------------------------

Empirical + theoretical reasons:

Reason 1 — More Capacity  
--------------------------------
A wider hidden layer captures richer patterns.

Reason 2 — Better Non-linear Mixing  
--------------------------------
Non-linear functions behave best in larger spaces.

Reason 3 — Universal Approximation  
--------------------------------
A 2-layer MLP with enough hidden units can approximate ANY function.

Reason 4 — Learned Abstractions  
--------------------------------
2048 dims help model lexical, syntactic, and semantic patterns.


9. FFN Summary (One Page)
-------------------------------------------

- FFN = 2-layer MLP inside every encoder/decoder layer  
- Formula: FFN(x) = ReLU(x W₁ + b₁) W₂ + b₂  
- Expands 512 → 2048 → 512  
- Adds non-linearity  
- Helps token-wise transformation  
- Works independently for each token  
- Essential for model expressiveness  
- Attention handles token interactions; FFN handles token transformations  



6. Encoder: Full Summary
===========================================

Each encoder layer performs:

    Input (512 dim per token)
        ↓
    Multi-Head Self Attention
        ↓
    Add & Norm
        ↓
    Feed Forward Network (2-layer MLP)
        ↓
    Add & Norm
        ↓
    Output (512 dim per token)

Stack N encoders → deep understanding of the entire sentence.


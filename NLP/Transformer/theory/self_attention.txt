Why Self-Attention is Needed in Transformers

1.The Problem with Old Models (RNNs/LSTMs)

RNNs and LSTMs process sentences sequentially â€” word by word. They read input in order (left to right), and each step depends on the previous one. This creates several problems:
A. They are slow
Because each step must wait for the previous one, the model cannot run in parallel. This makes training and inference slow.
B. They forget long-range relationships
Even LSTMs, which include mechanisms to handle long-term memory, struggle to remember information that appeared many words earlier. The influence of earlier words fades as sentences get longer.
C. They suffer from a bottleneck
At each step, the model must compress all previous context into a single hidden state vector. This bottleneck makes it difficult to store detailed or long-range information.

How Transformers Solve These Problems

Transformers introduce a new mechanism called Self-Attention, which removes recurrence entirely.
A. Parallel processing
Transformers process all words at once instead of step-by-step. This allows massive parallelization and greatly improves speed.
B. Self-Attention captures relationships between any words
Every word can directly connect to every other word, regardless of distance. This allows the model to understand long-range dependencies naturally and without information loss.
C. Positional encoding replaces recurrence
Since Transformers do not read words in order, they add positional encodings to each word embedding. This provides the model with information about the order of words in the sentence.


2.What Self-Attention Does

Self-Attention allows every word to look at all other words in the sentence and decide:
â€œWhich words are most relevant for understanding my meaning?â€
It lets the model pick out helpful context automatically.

Example:
Sentence: â€œApple launched a new phoneâ€

When processing the word â€œAppleâ€, the model must decide if it refers to a fruit or a company.
The word â€œphoneâ€ provides strong context, so â€œAppleâ€ pays more attention to it.
This helps the model understand that â€œAppleâ€ refers to the company, not the fruit.

Self-Attention basically allows words to use each other as clues, no matter how far apart they are.

3. Working of Self-Attention :

STATIC vs CONTEXTUAL EMBEDDINGS

What Are Static Embeddings?
Examples: Word2Vec, GloVe, FastText
Each word has one fixed vector.
â€œBankâ€ (financial) and â€œbankâ€ (river bank) â†’ same embedding.
â€œAppleâ€ (fruit) and â€œAppleâ€ (company) â†’ same vector.
No sentence awareness.

money  â†’ e_money
bank   â†’ e_bank
grows  â†’ e_grows
These embeddings never change regardless of the sentence.

Why Static Embeddings Are Not Enough

Word meanings depend on context:
â€œbankâ€ in money bank grows
â€œbankâ€ in river bank flows
Static embedding cannot tell the difference

Enter Self-Attention â†’ Contextual Embeddings

Self-Attention transforms each embedding based on how strongly it relates to other words in the same sentence.
The update rule is:
                ei(new)â€‹=jâˆ‘â€‹wijâ€‹â‹…Vjâ€‹

w_ij = attention weight from word i to word j
Vâ±¼ = Value vector of word j
(computed as ğ‘‰ğ‘— =ğ¸ğ‘—Ã—ğ‘Šv , not the static embedding)
This process turns fixed embeddings into context-aware embeddings, because each updated vector is a weighted mixture of the Value vectors of all other words in the sentence.


Sentence 1
money attends â†’ 0.7 money + 0.2 bank + 0.1 grows
bank  attends â†’ 0.25 money + 0.7 bank + 0.05 grows
grows attends â†’ 0.1 money + 0.2 bank + 0.7 grows

Contextual Embeddings (Sentence 1)
y_money = 0.7*V_money + 0.2*V_bank + 0.1*V_grows
y_bank  = 0.25*V_money + 0.7*V_bank + 0.05*V_grows
y_grows = 0.1*V_money + 0.2*V_bank + 0.7*V_grows

Sentence 2 

river attends â†’ 0.8 river + 0.15 bank + 0.05 flows
bank  attends â†’ 0.2 river + 0.78 bank + 0.02 flows
flows attends â†’ 0.4 river + 0.01 bank + 0.59 flows

Contextual Embeddings (Sentence 2)
y_river = 0.8*V_river + 0.15*V_bank + 0.05*V_flows
y_bank  = 0.2*V_river + 0.78*V_bank + 0.02*V_flows
y_flows = 0.4*V_river + 0.01*V_bank + 0.59*V_flows

Contextual embeddings solve the biggest linguistic problem:
One word = multiple meanings.
Transformers generate embeddings that depend on the entire sentence, not just the word itself.

Static embedding = dictionary meaning
Contextual embedding = meaning in the sentence (through Self-Attention)

Self-Attention answers one key question:
â€œWhen understanding this word, which other words should I pay attention to, and by how much?â€

It solves:
âœ… Long-range dependency
âœ… Multiple meanings (polysemy)
âœ… Parallel computation (no recurrence)
âœ… Context-aware representations


4. Introducing Q, K, V (Query, Key, Value)

Until now, we saw what Self-Attention does:
It lets each word look at all other words and decide how much to use them to understand its meaning.

But how does a word decide which other words are useful?
To make this decision, Transformers give three different versions of each word:

a- Query (Q) â€” What the word is looking for

This is like the word asking a question:
â€œWho can help me understand myself?â€
â€œWhat clues do I need?â€

Example:
If the word is Apple, its Query represents:
â€œHelp me decide if I'm a fruit or the company.â€

b- Key (K) â€” What each word offers

Every word also carries a â€œtagâ€ that represents what kind of information it contains.

Examples:
â€œphoneâ€ offers tech-related meaning
â€œtreeâ€ offers nature-related meaning
â€œmoneyâ€ offers financial meaning
â€œriverâ€ offers water/environment meaning
Keys tell other words:
â€œThis is what I represent.â€

c- Value (V) â€” The actual information each word provides

If a word is selected as important, its Value holds the information that gets added into the contextual meaning.
Think of Value as:
â€œIf you choose me, here is the meaning I will contribute.â€

Putting Q, K, V Together (A Simple Story)

Imagine we want to understand the meaning of the word bank.

Step 1 â€” â€œbankâ€ creates a Query
Its Query asks:
â€œAm I a financial bank or a river bank? What clues do I need?â€

Step 2 â€” All other words offer their Keys
â€œmoneyâ€ offers: â€œI'm related to finance.â€
â€œriverâ€ offers: â€œI'm related to nature/water.â€
â€œflowsâ€ offers: â€œI'm related to movement in water.â€

Step 3 â€” The Query of â€œbankâ€ matches each Key

Q(bank) matches strongly with K(money) â†’ financial context
Q(bank) matches weakly with K(grows) â†’ not helpful
Q(bank) matches strongly with K(river) â†’ water context

Step 4 â€” Only the useful words contribute their Values

If bank attends more to â€œmoneyâ€, its Value vector becomes financial
If it attends more to â€œriverâ€, its Value vector becomes nature-related

This is how:

â€œbankâ€ â†’ finance meaning in one sentence
â€œbankâ€ â†’ river meaning in another sentence

Q, K, V allow Self-Attention to determine meaning based on context.

Why Q, K, V Are Needed

Before this step, every word had only one embedding vector.
But each word has three roles inside attention:

It must ask things â†’ Query
It must represent itself â†’ Key
It must provide meaning â†’ Value

So the Transformer creates Q, K, V by applying three learnable transformations to the original word embedding.

This allows the model to learn:
What questions a word should ask (Q)
What information a word carries (K)
What meaning a word provides when selected (V)


5. How Attention Scores Are Calculated (Q, K â†’ Weights â†’ V)

Once every word has its Query (Q), Key (K), and Value (V), Self-Attention must determine which words matter for understanding the meaning of the current word.
This happens in three clear steps.

Q = E Ã— W_Q
K = E Ã— W_K
V = E Ã— W_V

These matrices are trained along with the whole Transformer.

5.1 Step 1 â€” Similarity (Q compared with K)

To understand a specific word (say â€œriverâ€), the model compares:
Q_river â†’ what â€œriverâ€ is looking for
with
K_river, K_bank, K_flows â†’ what each word offers

This comparison produces raw similarity scores, which answer:
â€œHow relevant is each word to â€˜riverâ€™?â€

sim(river, river)
sim(river, bank)
sim(river, flows)

5.2 Step 2 â€” Softmax (Scores â†’ Attention Weights)

Raw scores can be:
negative
very large
unbounded

Softmax converts these into normalized weights:
positive values
sum to 1
represent â€œhow much attention each word deservesâ€

w_river,river = 0.80
w_river,bank  = 0.15
w_river,flows = 0.05

These weights come directly from the Qâ€“K similarity.

5.3 Step 3 â€” Weighted Sum (THIS IS WHERE V IS USED)

Once attention weights are known, the model gathers information from the other words.
This information is taken from each wordâ€™s Value vector (V).

Each Value vector is computed as:
V_word = E_word Ã— W_V

y_river = 0.80 * V_river
        + 0.15 * V_bank
        + 0.05 * V_flows

This is the moment where:
Q asked the question
K decided relevance
V provided the actual meaning

The final Self-Attention output is a linear combination of Value vectors because attention weights represent how much each word should contribute.

A weighted sum is:
the natural way to aggregate information,
smooth and differentiable (necessary for training),
stable,
and allows contextual blending of meanings.

Q identifies what information is needed,
K tells how relevant each word is,
V provides the actual meaning,
and the linear combination mixes these meanings proportionally.


6. Why We Divide by âˆšdâ‚– (Scaled Dot-Product Attention)

After calculating similarity scores using Q Â· K, the Transformer applies Softmax to convert these scores into attention weights.
But before sending these scores to Softmax, we must scale them down.

Why?
Because without scaling, Self-Attention becomes unstable.

6.1 The Core Problem: Dot-Products Become Too Large

When Q and K are high-dimensional vectors (e.g., 64, 128, 512 dimensions), their dot-product naturally grows large.
Example:
If dâ‚– = 64, dot-products might be around ~8
If dâ‚– = 512, dot-products might explode to ~22 or more
Large dot-products cause issues:

A. Softmax Becomes Too Sharp
If the input numbers are large, Softmax produces outputs like:
[0.99999, 0.00001, 0.00000]
One word gets almost all the attention
Other words get almost zero
The model stops learning useful patterns
Gradients vanish (bad for training)

B. Attention Becomes Overconfident
The system picks one word as â€œthe only important oneâ€
Subtle relationships are lost
Meaning becomes brittle and unstable

6.2 The Solution: Scale the Dot-Product by âˆšdâ‚–

To avoid exploding similarity scores, the Transformer divides by: âˆšdâ‚–
This ensures the values passed into Softmax stay in a reasonable range, no matter how large dâ‚– is.
Example:

without scaling:   QÂ·K = 22  â†’ Softmax = extremely sharp  
with scaling:      22 / âˆš512 â‰ˆ 0.97  â†’ Softmax = normal, stable

6.3 Why Specifically âˆšdâ‚–?

Because:
Dot-products of random vectors have variance proportional to dâ‚–
Dividing by âˆšdâ‚– normalizes the variance to ~1

This keeps the distribution stable, and training becomes smooth and reliable.

6.4 Intuitive Explanation

Think of Q and K as huge vectors.
More dimensions â†’ more random noise â†’ larger dot-product
Softmax hates large, spiky numbers
Scaling by âˆšdâ‚– â€œshrinksâ€ the numbers to a manageable size
So the system keeps behaving properly regardless of embedding size.

6.5 Summary for Notes

Dot-products grow with vector dimension, which causes Softmax to produce overly sharp, unstable attention weights.
Dividing by âˆšdâ‚– scales the values back to a healthy range, stabilizing learning and ensuring smooth gradients.
This simple trick is why Self-Attention works reliably even with very large vector sizes.

Why do we divide by âˆšdâ‚– and not âˆšd_q or âˆšd_v?

Because the dot-product is computed between Q and K, and its variance grows with the size of K (which is the same as the size of Q).
Values (V) never enter the dot-product, so d_v does not matter.

The variance of this dot-product depends on how many terms are in the sum â†’ that number is d_k
So the dot-product naturally grows in magnitude with dâ‚–.
Thatâ€™s why we scale by âˆšdâ‚–.

In practice: d_q = d_k always (by design)
In Transformers:
The Query and Key vectors must have the same dimension
(otherwise QÂ·K wouldnâ€™t even be possible)

So d_q = d_k.

6.6 Summary for Notes

Dot-products between Q and K grow with vector dimension dâ‚–.
Large dot-products â†’ Softmax becomes overly sharp and unstable.
Dividing by âˆšdâ‚– scales similarity scores back to a healthy range.
This keeps gradients smooth and ensures stable learning.
âˆšdâ‚– is used because QÂ·K contains dâ‚– terms.
d_v is irrelevant because V is not in the dot-product.
d_q = d_k always, so âˆšdâ‚– is the standard choice.
